# LLM Council Environment Configuration
# Copy this file to .env and fill in your values

# Required: Your OpenRouter API key
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Optional: Council member models (JSON array)
# Default: ["openai/gpt-4o", "google/gemini-2.0-flash-exp", "anthropic/claude-3.5-sonnet", "x-ai/grok-2-1212"]
# COUNCIL_MODELS=["openai/gpt-4o", "google/gemini-2.0-flash-exp", "anthropic/claude-3.5-sonnet"]

# Optional: Chairman model for final synthesis
# Default: google/gemini-2.0-flash-exp
# CHAIRMAN_MODEL=openai/gpt-4o

# Optional: OpenRouter API URL
# Default: https://openrouter.ai/api/v1/chat/completions
# OPENROUTER_API_URL=https://openrouter.ai/api/v1/chat/completions

# Optional: Data directory for conversation storage
# Default: data/conversations
# DATA_DIR=data/conversations

# Optional: API timeout in seconds
# Default: 120
# API_TIMEOUT=120

# Optional: Max retries for failed API calls
# Default: 3
# API_MAX_RETRIES=3

# Optional: Server port for backend
# Default: 8001
# SERVER_PORT=8001

# Optional: CORS allowed origins (JSON array)
# Default: ["http://localhost:5173", "http://localhost:3000"]
# CORS_ORIGINS=["http://localhost:5173", "http://localhost:3000"]

# Frontend environment variables (for Vite)
# These go in frontend/.env or frontend/.env.local
# VITE_API_BASE=http://localhost:8001
